---
title:  目标检测数据集综述
tags: [Study]
mathjax: true
categories: AI
---

# 目标检测数据集综述



## 一、摘要

​		本文主要参考了[1]的博客，还有知乎[2]上的一篇文章，除此之外参考了一些领域内的论文与综述，[3],[4],[5]等，本文写作目的旨在为自己构建一个完整的目标检测知识体系架构，同时了解目标检测相关的技术栈及其未来的发展趋势

## 二、背景

​		计算机视觉问题主要分为四个：图像分类、目标检测、语义分割以及实例分割。作为计算机视觉的基本问题之一，目标检测构成了许多其他视觉任务的基础，例如实例分割，图像标注和目标跟踪等；从检测应用的角度看：行人检测、面部检测、文本检测、交通标注与红绿灯检测，遥感目标检测统称为目标检测的五大应用。

## 三、任务描述

​		目标检测（Object Detection）的目的是“识别目标并给出其在图中的确切位置”，其内容可解构为三部分：识别某个目标（Classification）；给出目标在图中的位置（Localization）；识别图中所有的目标及其位置（Detection）。从这三点可以看出目标检测的难度要比图像分类大很多，后者只需要确定输入的图像属于哪一类即可，而前者需要从图像中自动抠出（crop）合适大小的patch，并确定它的类别。其中抠出patch这一步难度很大，因为1：图像中目标个数不确定；2：目标的尺度和形状（大小和宽高比）不确定。

​		传统的目标检测主要是采用滑动窗口法：利用一系列不同大小和宽高比的滑动窗口，在图像上以一定步长来crop，从而得到一堆的小图；然后将每个小图送给后面的分类器（比如SVM或CNN）进行分类；之后再通过NMS来过滤掉重复的预测框。这个方法很经典，即从所有可能的小图中过滤三次，留下来的就是检测到的目标。后来的许多目标检测算法也都是从这里做进一步的改进。

​		滑动窗口法的缺陷也很明显：

- 定位不准。第一步利用不同大小和宽高比的滑动窗口来过滤时，只能得到固定大小和宽高比的小窗口，所以绝大部分目标的尺寸都会有一定的偏移（框的不太准）。
- 难以同时检测距离较近的目标。
- 计算量太大。这个是最大的问题，为了检测出更多不同尺寸的目标，为了定位更准确，为了检测出距离较近的目标，都不得不增加滑动窗口的个数，这就意味着超高的计算成本。

​		当前目标检测算法主要有两个分支，两段式检测算法和一段式检测算法：

- 两段式典型算法：R-CNN，SPP，Fast R-CNN，Faster R-CNN，R-FCN，Mask R-CNN
- 一段式典型算法：YOLO，SSD

​		两段式目标检测算法（比如R-CNN系列）的改进思路大方向主要有两点：

- 不再暴力的把所有小窗口都拿来做分类，而是先利用某些方法，从一大堆的小窗口中选出更可能是目标的候选框（region proposal），然后再拿去做分类。这就在一定程度上减少了计算成本。
- 粗略选出的候选框即便包含目标，也很可能有偏移，因此采用边框回归这一神器来校正窗口的位置。

​		而一段式中SSD算法则更加直接，直接用不同大小和宽高比的锚框（anchor box，也叫预设框，default box）将图像密密麻麻的铺满，然后再分别拿去做分类。这个思路实际上和滑动窗口法很相似，但高明之处在于：

- 只利用CNN提取一次特征，把特征图上各个像素和原图上预设的锚框联系起来，因此拿去做分类的并不是原图中crop出来的小窗口，而是特征图上各个像素对应的特征向量（这些特征向量就代表了对应感受野的局部语义特征）。
- 和两段式算法一样，也利用边框回归器来校正锚框的准确位置。

​		从这些方向可以看出，当前目标检测算法的主要思想是：在CNN提取图像特征的基础上，有针对性的crop部分窗口（或者预设特定尺寸的锚框）+边框回归器修正边框位置。

## 四、数据集描述

### （一）、**PASCAL VOC**

#### 1、简介

​		PASCAL VOC挑战赛 （The PASCAL Visual Object Classes ）是一个世界级的计算机视觉挑战赛, PASCAL全称：Pattern Analysis, Statical Modeling and Computational Learning，是一个由欧盟资助的网络组织。

​		很多优秀的计算机视觉模型比如分类，定位，检测，分割，动作识别等模型都是基于PASCAL VOC挑战赛及其数据集上推出的，尤其是一些目标检测模型（比如大名鼎鼎的R CNN系列，以及后面的YOLO，SSD等）。

​		PASCAL VOC从2005年开始举办挑战赛，每年的内容都有所不同，从最开始的分类，到后面逐渐增加检测，分割，人体布局，动作识别（Object Classification 、Object Detection、Object Segmentation、Human Layout、Action Classification）等内容，数据集的容量以及种类也在不断的增加和改善。该项挑战赛催生出了一大批优秀的计算机视觉模型（尤其是以深度学习技术为主的）。

![](image-20221204163708504.png)

![](image-20221204163730386.png)

​		在 ImageNet挑战赛上涌现了一大批优秀的分类模型，而PASCAL挑战赛上则是涌现了一大批优秀的目标检测和分割模型，这项挑战赛已于2012年停止举办了，但是研究者仍然可以在其服务器上提交预测结果以评估模型的性能。

​		近期的目标检测或分割模型更倾向于使用MS COCO数据集，但是这丝毫不影响 PASCAL VOC数据集的重要性，毕竟PASCAL对于目标检测或分割类型来说属于先驱者的地位。对于现在的研究者来说比较重要的两个年份的数据集是 PASCAL VOC 2007 与 PASCAL VOC 2012，这两个数据集频频在现在的一些检测或分割类的论文当中出现。

- [PASCAL主页](http://host.robots.ox.ac.uk/pascal/VOC/) 与 [排行榜](http://host.robots.ox.ac.uk:8080/leaderboard/main_bootstrap.php) （榜上已几乎看不到传统的视觉模型了，全是基于深度学习的）

- [PASCAL VOC 2007 挑战赛主页](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/) 与 [PASCAL VOC 2012 挑战赛主页](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/) 与 [PASCAL VOC Evaluation Server](http://host.robots.ox.ac.uk:8080/).

- 以及在两个重要时间点对 PASCAL VOC挑战赛 成绩进行总结的两篇论文

  - **The PASCAL Visual Object Classes Challenge: A Retrospective**

    Everingham, M., Eslami, S. M. A., Van Gool, L., Williams, C. K. I., Winn, J. and Zisserman, A.
    *International Journal of Computer Vision, 111(1), 98-136, 2015*
    [Bibtex source](http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham15.html#bibtex) | [Abstract](http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham15.html#abstract) | [PDF](http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham15.pdf)

    主要总结PASCAL VOC 2012的数据集情况，以及2011年-2013年之间出现的模型及其性能对比

  - **The PASCAL Visual Object Classes (VOC) Challenge**
    Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J. and Zisserman, A.
    *International Journal of Computer Vision, 88(2), 303-338, 2010*
    [Bibtex source](http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.html#bibtex) | [Abstract](http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.html#abstract) | [PDF](http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf)

    主要总结PASCAL VOC 2007的数据集情况，以及2008年之前出现的模型及其性能对比

  - 不过在以上论文中出现的深度学习模型只有一个 R-CNN吧，大部分都是传统方式的模型，毕竟深度学习模型主要在14年以后才大量涌现。

#### 2、数据集整体概况

##### 2.1 、层级结构

PASCAL VOC 数据集的20个类别及其层级结构：

![](image-20221204163951510.png)

- 从2007年开始，PASCAL VOC每年的数据集都是这个层级结构
- 总共四个大类：vehicle,household,animal,person
- 总共20个小类，预测的时候是只输出图中黑色粗体的类别
- **数据集主要关注分类和检测，也就是分类和检测用到的数据集相对规模较大。**关于其他任务比如分割，动作识别等，其数据集一般是分类和检测数据集的子集。

##### 2.2、 发展历程与使用方法

简要提一下在几个关键时间点数据集的一些关键变化，详细的请查看[PASCAL VOC主页](http://host.robots.ox.ac.uk/pascal/VOC/) 。

- 2005年：还只有4个类别： bicycles, cars, motorbikes, people. Train/validation/test共有图片1578 张，包含2209 个已标注的目标objects.

- **2007年 ：在这一年PASCAL VOC初步建立成一个完善的数据集。类别扩充到20类，Train/validation/test共有9963张图片，包含24640 个已标注的目标objects.**

  **07年之前的数据集中test部分都是公布的，但是之后的都没有公布。**

- 2009年：从这一年开始，通过在前一年的数据集基础上增加新数据的方式来扩充数据集。比如09年的数据集是包含了08年的数据集的，也就是说08年的数据集是09年的一个子集，以后每年都是这样的扩充方式，直到2012年；09年之前虽然每年的数据集都在变大（08年比07年略少），但是每年的数据集都是不一样的，也就是说每年的数据集都是互斥的，没有重叠的图片。

- **2012年：从09年到11年，数据量仍然通过上述方式不断增长，11年到12年，用于分类、检测和person layout 任务的数据量没有改变。主要是针对分割和动作识别，完善相应的数据子集以及标注信息。**

对于分类和检测来说，也就是下图所示的发展历程，相同颜色的代表相同的数据集：

![](image-20221204164051915.png)

分割任务的数据集变化略有不同：

- VOC 2012用于分类和检测的数据包含 2008-2011年间的所有数据，并与VOC2007互斥。
- VOC 2012用于分割的数据中train+val包含 2007-2011年间的所有数据，test包含2008-2011年间的数据，没有包含07年的是因为07年的test数据已经公开了。

​		2012年是最后一次挑战赛，最终用于分类和检测的数据集规模为：train/val ：11540 张图片，包含 27450 个已被标注的 ROI annotated objects ；用于分割的数据集规模为：trainval：2913张图片，6929个分割，用于其他任务的不再细说，[参考这里](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/dbstats.html) 。

​		即便挑战赛结束了，但是研究者们仍然可以上传预测结果进行评估。上传入口： [PASCAL VOC Evaluation Server](http://host.robots.ox.ac.uk:8080/).

​		**目前广大研究者们普遍使用的是 VOC2007和VOC2012数据集，因为二者是互斥的，不相容的。**

​		**论文中针对 VOC2007和VOC2012 的具体用法有以下几种：**

- 只用VOC2007的trainval 训练，使用VOC2007的test测试
- 只用VOC2012的trainval 训练，使用VOC2012的test测试，这种用法很少使用，因为大家都会结合VOC2007使用
- 使用 VOC2007 的 train+val 和 VOC2012的 train+val 训练，然后使用 VOC2007的test测试，这个用法是论文中经常看到的 07+12 ，研究者可以自己测试在VOC2007上的结果，因为VOC2007的test是公开的。
- 使用 VOC2007 的 train+val+test 和 VOC2012的 train+val训练，然后使用 VOC2012的test测试，这个用法是论文中经常看到的 07++12 ，这种方法需提交到VOC官方服务器上评估结果，因为VOC2012 test没有公布。
- 先在 MS COCO 的 trainval 上预训练，再使用 VOC2007 的 train+val、 VOC2012的 train+val 微调训练，然后使用 VOC2007的test测试，这个用法是论文中经常看到的 07+12+COCO 。
- 先在 MS COCO 的 trainval 上预训练，再使用 VOC2007 的 train+val+test 、 VOC2012的 train+val 微调训练，然后使用 VOC2012的test测试 ，这个用法是论文中经常看到的 07++12+COCO，这种方法需提交到VOC官方服务器上评估结果，因为VOC2012 test没有公布。

​		在各自数据集上分别进行建模和评测的用法比较少，基本上在早期论文里出现就是起个对照作用；现在的大部分论文都会为了增加数据量而将二者合起来使用。

#### 3、数据量概述

由于现在的研究基本上都是在VOC2007和VOC2012上面进行，因此只介绍这两个年份的。

##### 3.1、VOC 2007

一些示例图片展示：[Classification/detection example images](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/examples/index.html)

**数据集总体统计：**

![](image-20221204164345888.png)

- **以上是数据集总体的统计情况，这个里面是包含了测试集的，可见person 类是最多的。**

**训练集，验证集，测试集划分情况**

![](image-20221204164422204.png)

- PASCAL VOC 2007 数据集分为两部分：训练和验证集trainval，测试集test ，两部分各占数据总量的约 50%。其中trainval 又分为训练集和测试集，二者分别各占trainval的50%。
- 每张图片中有可能包含不只一个目标object。

这里只贴出用于分类和检测的划分情况，关于分割或者其他任务的划分方式 [点击这里查看](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/dbstats.html) 。

##### 3.2、 VOC 2012

一些示例图片展示：[Classification/detection example images](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/examples/index.html)

**数据集总体统计**：

![](image-20221204164556400.png)

- **这个统计是没有包含 test部分的，仍然是person类最多**

**trainval部分的数据统计：**

![](image-20221204164620611.png)

test部分没有公布，同样的 除了分类和检测之外的数据统计，[参考这里](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/dbstats.html)

##### 3.3、VOC 2007 与 2012 的对比

VOC 2007 与 2012 数据集及二者的并集 数据量对比

![](image-20221204164726974.png)

- 黑色字体所示数字是官方给定的，由于VOC2012数据集中 test 部分没有公布，因此红色字体所示数字为估计数据，按照PASCAL 通常的划分方法，即 trainval 与test 各占总数据量的一半。

#### 4、标注信息

数据集的标注还是很谨慎的，有专门的标注团队，并遵从统一的标注标准，参考 [guidelines](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/guidelines.html) 。

标注信息是用 xml 文件组织的如下：

```xml
<annotation>
	<folder>VOC2007</folder>
	<filename>000001.jpg</filename>
	<source>
		<database>The VOC2007 Database</database>
		<annotation>PASCAL VOC2007</annotation>
		<image>flickr</image>
		<flickrid>341012865</flickrid>
	</source>
	<owner>
		<flickrid>Fried Camels</flickrid>
		<name>Jinky the Fruit Bat</name>
	</owner>
	<size>
		<width>353</width>
		<height>500</height>
		<depth>3</depth>
	</size>
	<segmented>0</segmented>
	<object>
		<name>dog</name>
		<pose>Left</pose>
		<truncated>1</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>48</xmin>
			<ymin>240</ymin>
			<xmax>195</xmax>
			<ymax>371</ymax>
		</bndbox>
	</object>
	<object>
		<name>person</name>
		<pose>Left</pose>
		<truncated>1</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>8</xmin>
			<ymin>12</ymin>
			<xmax>352</xmax>
			<ymax>498</ymax>
		</bndbox>
	</object>
</annotation>
```

- filename ：文件名
- source，owner：图片来源，及拥有者
- size：图片大小
- segmented：是否分割
- object：表明这是一个目标，里面的内容是目标的相关信息
- name：object名称，20个类别
- pose：拍摄角度：front, rear, left, right, unspecified
- truncated：目标是否被截断（比如在图片之外），或者被遮挡（超过15%）
- difficult：检测难易程度，这个主要是根据目标的大小，光照变化，图片质量来判断
- difficult 标签示例：图中白色虚线，被标记为 difficult。
- bndbox：bounding box 的左上角点和右下角点的4个坐标值。

![](image-20221204164932692.png)

#### 5、数据集组织结构

数据集的下载:

```
# Download the data.
cd $HOME/data
wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar
wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar
wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar
# Extract the data.
tar -xvf VOCtrainval_11-May-2012.tar
tar -xvf VOCtrainval_06-Nov-2007.tar
tar -xvf VOCtest_06-Nov-2007.tar
```

或者支直接点击下面链接下载：

- Download the [training/validation data](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar) (450MB tar file)
- Download the [annotated test data](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar) (430MB tar file)

上面的解压命令会将VOC2007的trainval和test解压到一块，数据会混在一起，如果不想，可以自己指定解压路径。以VOC 2007 为例，解压后的文件：

```
.
├── Annotations 进行detection 任务时的 标签文件，xml文件形式
├── ImageSets 存放数据集的分割文件，比如train，val，test
├── JPEGImages 存放 .jpg格式的图片文件
├── SegmentationClass 存放 按照class 分割的图片
└── SegmentationObject 存放 按照 object 分割的图片
```

**Annotations 文件夹：**

```
.
├── 000001.xml
├── 000002.xml
├── 000003.xml
├── 000004.xml
……
……
……
├── 009962.xml
└── 009963.xml

```

​		以xml 文件的形式，存放标签文件，文件内容如前述，文件名与图片名是一样的，6位整数

**ImageSets文件夹：**

​		存放数据集的分割文件

​		包含三个子文件夹 Layout，Main，Segmentation，其中Main文件夹存放的是用于分类和检测的数据集分割文件，Layout文件夹用于 person layout任务，Segmentation用于分割任务

```
.
├── Layout
│   ├── test.txt
│   ├── train.txt
│   ├── trainval.txt
│   └── val.txt
├── Main
│   ├── aeroplane_test.txt
│   ├── aeroplane_train.txt
│   ├── aeroplane_trainval.txt
│   ├── aeroplane_val.txt
│   ├── bicycle_test.txt
│   ├── bicycle_train.txt
│   ├── bicycle_trainval.txt
│   ├── bicycle_val.txt
│   ├── bird_test.txt
│   ├── bird_train.txt
│   ├── bird_trainval.txt
│   ├── bird_val.txt
│   ├── boat_test.txt
│   ├── boat_train.txt
│   ├── boat_trainval.txt
│   ├── boat_val.txt
│   ├── bottle_test.txt
│   ├── bottle_train.txt
│   ├── bottle_trainval.txt
│   ├── bottle_val.txt
│   ├── bus_test.txt
│   ├── bus_train.txt
│   ├── bus_trainval.txt
│   ├── bus_val.txt
│   ├── car_test.txt
│   ├── car_train.txt
│   ├── car_trainval.txt
│   ├── car_val.txt
│   ├── cat_test.txt
│   ├── cat_train.txt
│   ├── cat_trainval.txt
│   ├── cat_val.txt
│   ├── chair_test.txt
│   ├── chair_train.txt
│   ├── chair_trainval.txt
│   ├── chair_val.txt
│   ├── cow_test.txt
│   ├── cow_train.txt
│   ├── cow_trainval.txt
│   ├── cow_val.txt
│   ├── diningtable_test.txt
│   ├── diningtable_train.txt
│   ├── diningtable_trainval.txt
│   ├── diningtable_val.txt
│   ├── dog_test.txt
│   ├── dog_train.txt
│   ├── dog_trainval.txt
│   ├── dog_val.txt
│   ├── horse_test.txt
│   ├── horse_train.txt
│   ├── horse_trainval.txt
│   ├── horse_val.txt
│   ├── motorbike_test.txt
│   ├── motorbike_train.txt
│   ├── motorbike_trainval.txt
│   ├── motorbike_val.txt
│   ├── person_test.txt
│   ├── person_train.txt
│   ├── person_trainval.txt
│   ├── person_val.txt
│   ├── pottedplant_test.txt
│   ├── pottedplant_train.txt
│   ├── pottedplant_trainval.txt
│   ├── pottedplant_val.txt
│   ├── sheep_test.txt
│   ├── sheep_train.txt
│   ├── sheep_trainval.txt
│   ├── sheep_val.txt
│   ├── sofa_test.txt
│   ├── sofa_train.txt
│   ├── sofa_trainval.txt
│   ├── sofa_val.txt
│   ├── test.txt
│   ├── train_test.txt
│   ├── train_train.txt
│   ├── train_trainval.txt
│   ├── train.txt
│   ├── train_val.txt
│   ├── trainval.txt
│   ├── tvmonitor_test.txt
│   ├── tvmonitor_train.txt
│   ├── tvmonitor_trainval.txt
│   ├── tvmonitor_val.txt
│   └── val.txt
└── Segmentation
    ├── test.txt
    ├── train.txt
    ├── trainval.txt
    └── val.txt
    
3 directories, 92 files
```

​		主要介绍一下Main文件夹中的组织结构，先来看以下这几个文件：

```
├── Main
│   ├── train.txt 写着用于训练的图片名称 共2501个
│   ├── val.txt 写着用于验证的图片名称 共2510个
│   ├── trainval.txt train与val的合集 共5011个
│   ├── test.txt 写着用于测试的图片名称 共4952个
```

​		里面的文件内容是下面这样的：以train.txt文件为例

```
000012
000017
000023
000026
000032
000033
000034
000035
000036
000042
……
……
009949
009959
009961
```

就是对数据库的分割，这一部分图片用于train，其他的用作val，test等。

​		Main中剩下的文件很显然就是每一类别在train或val或test中的ground truth，这个ground truth是为了方便classification 任务而提供的；如果是detection的话，使用的是上面的xml标签文件。

```
├── Main
│   ├── aeroplane_test.txt 写着用于训练的图片名称 共2501个，指定正负样本
│   ├── aeroplane_train.txt 写着用于验证的图片名称 共2510个，指定正负样本
│   ├── aeroplane_trainval.txt train与val的合集 共5011个，指定正负样本
│   ├── aeroplane_val.txt 写着用于测试的图片名称 共4952个，指定正负样本
……
……
```

​		里面文件是这样的（以aeroplane_train.txt为例）：

```
000012 -1
000017 -1
000023 -1
000026 -1
000032  1
000033  1
000034 -1
000035 -1
000036 -1
000042 -1
……
……
009949 -1
009959 -1
009961 -1
```

​		前面一列是训练集中的图片名称，这一列跟train.txt文件中的内容是一样的，后面一列是标签，即训练集中这张图片是不是aeroplane，是的话为1，否则为-1.

​		其他所有的 (class)_(imgset).txt 文件都是类似的。

- (class)_train 存放的是训练使用的数据，每一个class都有2501个train数据。
- (class)_val 存放的是验证使用的数据，每一个class都有2510个val数据。
- (class)_trainval 将上面两个进行了合并，每一个class有5011个数据。
- (class)_test 存放的是测试使用的数据，每一个class有4952个test数据。

​		所有文件都 指定了正负样本，每个class的实际数量为正样本的数量，train和val两者没有交集。

​		VOC2012 的数据集组织结构是类似的，不一样的地方在于VOC2012 中没有 test类的图片和以及相关标签和分割文件，因为这部分数据 VOC2012没有公布。

### （二）、**ILSVRC**

#### 1、简介

从2010年开始,每年举办的ILSVRC图像分类和目标检测大赛。

Imagenet数据集是目前深度学习图像领域应用得非常多的一个领域，关于图像分类、定位、检测等研究工作大多基于此数据集展开。

Imagenet数据集文档详细，有专门的团队维护，使用非常方便，在计算机视觉领域研究论文中应用非常广，几乎成为了目前深度学习图像领域算法性能检验的“标准”数据集。

Imagenet数据集有1400多万幅图片，涵盖2万多个类别；

其中有超过百万的图片有明确的类别标注和图像中物体位置的标注。 

**具体信息如下：**

Total number of non-empty synsets: 21841

Total number of images: 14,197,122

Number of images with bounding boxannotations: 1,034,908

Number of synsets with SIFT features: 1000

Number of images with SIFT features: 1.2million

Imagenet数据集是一个非常优秀的数据集，但是标注难免会有错误，几乎每年都会对错误的数据进行修正或是删除，建议下载最新数据集并关注数据集更新。

![](image-20221204165630523.png)

#### 2、下载链接

官网：http://www.image-net.org/

数据集下载地址：http://www.image-net.org/challenges/LSVRC/

![](image-20221204165751913.png)

#### 3、应用场景

![](image-20221204165816900.png)

![](image-20221204165827215.png)

![](image-20221204165840801.png)

![](image-20221204165847983.png)

![](image-20221204165855821.png)





### （三）、**MS-COCO**

#### 1、简介

​		MS COCO的全称是Microsoft Common Objects in Context，起源于微软于2014年出资标注的Microsoft COCO数据集，与ImageNet竞赛一样，被视为是计算机视觉领域最受关注和最权威的比赛之一。 
​        COCO数据集是一个大型的、丰富的物体检测，分割和字幕数据集。这个数据集以scene understanding为目标，主要从复杂的日常场景中截取，图像中的目标通过精确的segmentation进行位置的标定。图像包括91类目标，328,000影像和2,500,000个label。目前为止有语义分割的最大数据集，提供的类别有80 类，有超过33 万张图片，其中20 万张有标注，整个数据集中个体的数目超过150 万个。

![](image-20221204170015296.png)

**官网地址**：[http://cocodataset.org](http://cocodataset.org/#home)

#### 2、COCO数据集的80个类别—[YoloV3](https://so.csdn.net/so/search?q=YoloV3&spm=1001.2101.3001.7020)算法采用的数据集

person(人) 
bicycle(自行车)  car(汽车)  motorbike(摩托车)  aeroplane(飞机)  bus(公共汽车)  train(火车)  truck(卡车)  boat(船) 
traffic light(信号灯)  fire hydrant(消防栓)  stop sign(停车标志)  parking meter(停车计费器)  bench(长凳) 
bird(鸟)  cat(猫)  dog(狗)  horse(马)  sheep(羊)  cow(牛)  elephant(大象)  bear(熊)  zebra(斑马)  giraffe(长颈鹿) 
backpack(背包)  umbrella(雨伞)  handbag(手提包)  tie(领带)  suitcase(手提箱) 
frisbee(飞盘)  skis(滑雪板双脚)  snowboard(滑雪板)  sports ball(运动球)  kite(风筝) baseball bat(棒球棒)  baseball glove(棒球手套)  skateboard(滑板)  surfboard(冲浪板)  tennis racket(网球拍) 
bottle(瓶子)  wine glass(高脚杯)  cup(茶杯)  fork(叉子)  knife(刀)
spoon(勺子)  bowl(碗) 
banana(香蕉)  apple(苹果)  sandwich(三明治)  orange(橘子)  broccoli(西兰花)  carrot(胡萝卜)  hot dog(热狗)  pizza(披萨)  donut(甜甜圈)  cake(蛋糕)
chair(椅子)  sofa(沙发)  pottedplant(盆栽植物)  bed(床)  diningtable(餐桌)  toilet(厕所)  tvmonitor(电视机) 
laptop(笔记本)  mouse(鼠标)  remote(遥控器)  keyboard(键盘)  cell phone(电话) 
microwave(微波炉)  oven(烤箱)  toaster(烤面包器)  sink(水槽)  refrigerator(冰箱)
book(书)  clock(闹钟)  vase(花瓶)  scissors(剪刀)  teddy bear(泰迪熊)  hair drier(吹风机)  toothbrush(牙刷)

#### 3、COCO数据集

​     该数据集主要解决3个问题：目标检测，目标之间的上下文关系，目标的2维上的精确定位。COCO数据集有91类，虽然比ImageNet和SUN类别少，但是每一类的图像多，这有利于获得更多的每类中位于某种特定场景的能力，对比PASCAL VOC，其有更多类和图像。

##### 3.1、COCO目标检测挑战

COCO数据集包含20万个图像；
80个类别中有超过50万个目标标注,它是最广泛公开的目标检测数据库；
平均每个图像的目标数为7.2，这些是目标检测挑战的著名数据集。

##### 3.2、COCO数据集的特点

COCO is a large-scale object detection, segmentation, and captioning dataset. COCO has several features:

![](image-20221204170247180.png)

##### 3.3、数据集大小和版本

**大小：**

​				25 GB（压缩）
**记录数量：**

​				 330K图像、80个对象类别、每幅图像有5个标签、25万个关键点。

**版本：**		

​				COCO数据集分两部分发布，前部分于2014年发布，后部分于2015年，2014年版本：82,783 training, 40,504 validation, and 40,775 testing images，有270k的segmented people和886k的segmented object；2015年版本：165,482 train, 81,208 val, and 81,434 test images。

**2014年版本的数据：**

				一共有20G左右的图片和500M左右的标签文件。标签文件标记了每个segmentation的像素精确位置+bounding box的精确坐标，其精度均为小数点后两位。
##### 3.4、COCO数据集的下载

官网地址：http://cocodataset.org/#download↳

1、2014年数据集的下载
train2014：http://images.cocodataset.org/zips/train2014.zip
val2014：http://images.cocodataset.org/zips/val2014.zip
http://msvocds.blob.core.windows.net/coco2014/train2014.zip

2、2017的数据集的下载
http://images.cocodataset.org/zips/train2017.zip
http://images.cocodataset.org/annotations/annotations_trainval2017.zip
http://images.cocodataset.org/zips/val2017.zip
http://images.cocodataset.org/annotations/stuff_annotations_trainval2017.zip
http://images.cocodataset.org/zips/test2017.zip
http://images.cocodataset.org/annotations/image_info_test2017.zip

train2017	
train2017：http://images.cocodataset.org/zips/train2017.zip
train2017 annotations：http://images.cocodataset.org/annotations/annotations_trainval2017.zip

val2017	
val2017：http://images.cocodataset.org/zips/val2017.zip
val2017 annotations：http://images.cocodataset.org/annotations/stuff_annotations_trainval2017.zip

test2017	
test2017：http://images.cocodataset.org/zips/test2017.zip
test2017 info：http://images.cocodataset.org/annotations/image_info_test2017.zip



### （四）、**Open Images(QID)**

#### 1、简介

​		在过去几年中，机器学习的进步使计算机视觉得以快速发展，允许系统自动为应用程序添加图像字幕，从而创建自然语言回复，以响应共享的照片。这一进展的很大一部分可以归因于公开可用的图像数据集，例如用于监督学习的ImageNet和COCO，以及用于非监督学习的YFCC100M。 

​		Open Images是一个由约900万个URL组成的数据集，这些URL指向的图像已经标注了超过6000个类别的标签。我们试图使数据集尽可能实用：标签覆盖了比1000个ImageNet类更多的真实实体，有足够的图像从头开始训练深度神经网络，这些图像被列为具有Creative Commons Attribution许可证*。 

​		图像级别的注释已经用类似于GoogleCloudVisionAPI的视觉模型自动填充。对于验证集，我们让人工评分员验证这些自动标签，以发现并消除误报。平均而言，每个图像分配了大约8个标签。以下是一些示例：

![](image-20221204170912228.png)

​		我们已经训练了一个仅基于Open Images注释的Inception v3模型，该模型足够好，可以用于微调应用程序以及其他事情，如DeepDream或艺术风格转换，这需要一个完善的过滤器层次结构。我们希望在未来几个月提高Open Images中注释的质量，从而提高可以训练的模型的质量。 

​		该数据集是谷歌、CMU和康奈尔大学合作的产物，在开放图像数据集的基础上，还有许多研究论文正在研究中。我们希望像Open Images和最近发布的YouTube-8M这样的数据集将成为机器学习社区的有用工具。

#### 2、下载途径

Image URLs and metadata https://storage.googleapis.com/openimages/2016_08/images_2016_08_v2.tar.gz (654 MB MB)
Machine image-level annotations (train and validation sets) https://storage.googleapis.com/openimages/2016_08/machine_ann_2016_08.tar.gz (330 MB)
Human image-level annotations (validation set) https://storage.googleapis.com/openimages/2016_08/human_ann_2016_08.tar.gz (7 MB)

### （五）、**其他数据集**

## 五、算法简介

​	当前目标检测算法主要有两个分支，两段式检测算法和一段式检测算法，下面依次介绍。

​	由于资料中候选框和RoI(Region of Interest)这两个概念的使用比较混乱，为避免混淆，本文中候选框(region proposal)特指原图像中的候选框，RoI特指候选框对应特征图上的特征区域。

### 5.1 两段式检测算

​	两段式检测算法也称为基于区域（Region-based）的方法，主要思想是先选出一部分包含目标可能性较大的候选框，然后再做进一步的分类和边框回归，过程分为两个阶段，因此叫两段式。典型代表有R-CNN，SPP，Fast R-CNN，Faster R-CNN，R-FCN，Mask R-CNN，这些刚好也是按照时间顺序，在前人基础上一步一步发展起来的。

#### 5.1.1 R-CNN

![](1787018-20200419232200923-309543896.png)

​	[2013年由rbg提出，](https://arxiv.org/pdf/1311.2524.pdf)是利用卷积神经网络来做目标检测的开山之作，意义重大。

检测过程：

- 输入测试图像，利用选择性搜索Selective Search算法在图像中提取2000个左右的可能包含物体的候选框；
- 因为取出的候选框大小各不相同，所以需要将每个候选框缩放（warp）成统一的227x227的大小并输入到CNN，将CNN的fc7层的输出作为提取到的特征；
- 将每个候选框提取到的CNN特征输入到SVM进行分类；
- 对那些识别出目标的特征向量，送到边框回归器中校正窗口，生成预测窗口坐标。

训练过程：

- 步骤一，训练CNN：首先下载（或者训练）一个分类模型（比如AlexNet）；然后对模型进行微调，即将分类数从1000改为20，比如20个物体类别 + 1个背景，训练完之后去掉最后一个全连接层。
- 步骤二，提取特征：提取图像的所有候选框（选择性搜索Selective Search）；针对每一个区域，将每个候选框缩放（warp）后输入到CNN，将第五个池化层的输出（就是对候选框提取到的特征）存到硬盘。
- 步骤三，训练分类器：针对每一个类别，训练一个SVM分类器（二分类），是就是positive，反之nagative。
- 步骤四，训练边框回归器：针对每一个类别，训练一个对应的边框回归器。

注意：

- 针对每一个类别，都有一个SVM分类器，采用OVR方法完成多类别的分类；（林轩田老师在机器学习基石课上说这种方法不好，因为采用硬分类器的OVR会导致有些样本无法归类，或者被归为不同的类，所以应该用软分类LR）（个人理解：此处用SVM分类器，可能是图像特征空间维度特别大，所以各类别的特征之间离得很远，故不需担心这种问题）

R-CNN缺点：

- 训练分为多个阶段，需要微调网络+训练SVM+训练边框回归器，步骤繁琐：
- Selective Search选出来的候选框大小不一，都要经过缩放之后再提取特征，会丢失一些信息；
- 每张原图约有2000张候选框，每个都要进行特征提取，导致大量的重复计算（很多候选区域重叠），一方面耗时（使用GPU，VGG16模型处理一张图像需要47s），另一方面提取出的特征占用大量磁盘空间（5000张图像产生几百G的特征文件）。

#### 5.1.2 SPP-Net

![](1787018-20200419232354392-145078578.png)

[2014年何恺明提出SPP-Net](https://arxiv.org/pdf/1406.4729.pdf)，解决了R-CNN中两个大问题：一是每个候选框都要送到CNN中提取特征；二是每个候选框都要缩放成固定大小。SPP-Net利用CNN，只提取一次图像的特征，将各个候选框的坐标映射到特征图上，从而直接得到各个候选框的特征区域(ROI)，避免了R-CNN中对各个候选框重复地提取特征；使用空间金字塔池化，使得任意大小的ROI都能够转换成固定大小的特征向量。

**检测过程：**

- 第一步同R-CNN，利用Selective Search算法在图像中提取2000个左右的可能包含物体的候选框；
- 将原图输入ZF-5网络，提取出图像的特征（通道个数为256维）；
- 利用stride映射方法，将各个候选框坐标直接映射到特征图上，从而得到各个候选框对应的ROI；
- 利用空间金字塔网络，将上一步得到的ROI转换成固定大小的输出（12800维）；
- 将上一步得到的12800维向量接上fc6和fc7，之后去做分类和边框回归。

**空间金字塔池化的具体过程：**针对任意一张ROI，用一张6x6的格子模板将其切成36块；同理，继续用3x3、2x2、1x1的格子将其切分成不同尺度的块（总共可以切成50块），然后做最大池化，从而得到了50x256=12800维的特征向量。（上图为示意图，其中只有4x4、2x2、1x1的格子，因此输出的是21x256=5376维的特征向量）

**如何将原图候选框映射到特征图上：**在文章的附录A中直接给出了一个简单的计算公式，即对于top/left坐标 x′=⌊x/S⌋+1x′=⌊x/S⌋+1，对于bottom/right坐标 x′=⌈x/S⌉−1x′=⌈x/S⌉−1。其中x′x′表示特征图中的坐标，xx表示原图中坐标，S表示stride，即下采样倍数。

#### 5.1.3 Fast R-CNN

![](1787018-20200419232505467-2032366605.png)

[2015年rbg提出Fast R-CNN，](https://arxiv.org/abs/1504.08083)吸收了SPP-Net中这两个巧妙的解决方案，并针对损失函数做了优化。Fast R-CNN和R-CNN相比，训练时间从84小时减少到9.5小时，测试时间从47秒减少到0.32秒，并且在PASCAL VOC 2007上测试的准确率相差无几，约在66%-67%之间。

针对R-CNN的主要改进点：

- 吸收SPP Net的思想，只对原图做一次特征提取，然后将选择出来的候选区域映射到特征图上，直接从特征图上得到ROI；
- 将VGG最后一层池化层换成ROI池化层（即单层SPP），可以将不同尺度的ROI转化为固定的尺寸，便于和后面全连接层对接；
- 将分类器由SVM换成softmax，将边框回归器的损失换成smooth L1损失，然后将两个损失函数合并成一个多任务损失函数(multi-task loss)，将边框回归器和分类器放在一块训练，这样整个的训练过程是端到端的(除去Region Proposal提取阶段)。（smooth L1 loss让loss对于离群点更加鲁棒，即：相比于L2损失函数，其对离群点、异常值（outlier）不敏感，梯度变化相对更小，训练时不容易跑飞。）

#### 5.1.4 Faster R-CNN

![](1787018-20200419232541844-1046173644.png)

经过对R-CNN的改进，Fast R-CNN可以把处理一张图像的时间从47s降到0.32s（不包括提取候选框的时间），但Selective Search在原图像上提取候选框这一步的时间接近2s，所以整个检测环节的瓶颈就卡在这一步。于是，[2015年任少卿等人提出Faster R-CNN，直接把Selective Search提取候选框这一步换成RPN（Region Proposal NetWork）](https://arxiv.org/pdf/1506.01497.pdf)，**即Faster R-CNN = RPN + Fast R-CNN**。RPN(Region Proposal Networks)的结构很简单，只有两层卷积，利用一个分类器和一个边框回归器，直接从特征图上提取ROI，然后送到后面进一步细分。经过改进，Faster R-CNN将特征抽取(feature extraction)、ROI提取、边框回归、分类都整合在了一个网络中，把处理一张图像的时间从2s降到了0.2s（包括提取候选框的时间）。

**RPN运行机制：**

- 利用CNN（比如ZFNet、VGG16、ResNet）提取图像特征；
- 利用3x3卷积核对整张特征图进行卷积，得到尺寸不变的特征图，其中每个像素对应的256维特征向量代表了对应感受野的局部语义特征；每个像素点对应k个锚框，这里k=9，因此需要获得每个锚框的类别概率和偏移量；
- 对于每个256维特征向量，分别通过1x1卷积，得到2x9的分类结果和4x9的回归结果；
- 将RPN输出的前景和偏移量映射回原图，得到一堆的预测框，经过NMS和TopN处理后即可得到一堆的候选框；
- 将候选框坐标通过Fast R-CNN中stride映射的方式，map到整张特征图上，即可得到一堆候ROI。

![](1787018-20200419232610892-1493001722.png)

注意：

- RPN中使用3x3进行卷积，卷积结果对应的感受野：ZFNet是171，VGG16是228；而Faster R-CNN实际使用了3组大小(128×128、256×256、512×512)、3组长宽比(1:1、1:2、2:1)，共计9个锚框。换句话说，感受野大致和中等锚框的尺寸一致，但小于大锚框的尺寸，作者解释这样也是可行的。
- 基于ResNet101的Faster R-CNN，ROI池化层插在第四个卷积模块的输出后面，而非第五个，即利用第四个卷积模块的输出作为提取到的图像特征，所有的计算只共享前面91层(1+(3+4+23)x3=91)。
- 可参考[RPN 解析](https://blog.csdn.net/lanran2/article/details/54376126)，[一文读懂Faster RCNN](https://zhuanlan.zhihu.com/p/31426458).

#### 5.1.5 R-FCN

![](1787018-20200419232702827-1647893200.png)

Faster R-CNN在ROI pooling之后，需要对每个ROI单独进行两分支预测，比如基于ResNet101时，所有的计算只共享前面91层卷积，而抠出来的各个ROI都不得不单独跑一遍后10层卷积。[2016年微软戴季峰等人提出R-FCN，](https://arxiv.org/pdf/1605.06409v2.pdf)旨在使几乎所有的计算共享，以进一步加快速度。

**主要思路：**

- 对于region-based的检测方法，以Faster R-CNN为例，实际上是分成了几个subnetwork，第一个用来在整张图上做比较耗时的conv，这些操作与region无关，是计算共享的。第二个subnetwork是用来产生候选框（如RPN），第三个subnetwork用来分类或回归（如Fast RCNN），这个subnetwork和region是有关系的，必须每个region单独跑网络，衔接在这个subnetwork和前两个subnetwork中间的就是ROI pooling。我们希望的是，耗时的卷积都尽量移到前面共享的subnetwork上。
- 但是直接将ROI池化层放到ResNet101的第五个卷积模块后面会导致效果变差，因为网络太深，导致出来的特征图尺寸不够，难以携带足够的信息，特别是位置信息，于是设置一个位置敏感得分图，用来保留位置信息。
- 因此R-FCN的做法是，从conv4引出RPN获得ROI，再将ROI映射到得分图上，通过位置敏感ROI池化(position sensitive roi pooling)的方法获得ROI池化特征图。（Faster R-CNN是从conv4特征图上抠出ROI特征区域后，用普通ROI池化的方法得到ROI池化特征图的）

[具体细节可参考源码。](https://github.com/YuwenXiong/py-R-FCN/blob/master/models/pascal_voc/ResNet-101/rfcn_end2end/test_agnostic.prototxt)以k=7, C=20为例，假设ResNet的输出为100x100x2048，则数据流如下所示：

![](1787018-20200419232743066-12361021.png)

**R-FCN的网络结构：**

- 基础conv网络是ResNet101，将全局平均池化和fc层去掉，换成1x1的卷积层以减少通道数(由2048转成1024)，即这101层卷积网络是共享的。
- 针对上一步中101层网络的输出，采用k2∗(C+1)k2∗(C+1)个1x1卷积核来卷积，得到k2k2组特征图，其中每组有C+1个通道(即总通道数为1029)，即称为位置敏感得分图rfcn_cls，其宽高不变。另一条路径rfcn_bbox类似，输出通道数8x7x7=392。
- RPN从conv4的输出引出分类器和回归器，映射到原图上，得到一堆候选框。
- 将候选框映射到rfcn_cls上(实现时直接除以stride=16)，并通过位置敏感ROI池化的方法，得到大小为7x7，通道数为21的ROI池化特征图，再经过全局平均池化和softmax之后输出分类结果；另一路类似，得到回归结果。
- 位置敏感ROI池化和Faster R-CNN中普通ROI池化不同，后者只需要把ROI切成7x7个格子，每个格子做最大池化即可，而前者将ROI切成7x7个格子后，每个格子取自不同的组，然后再做池化(如第一幅图中位置敏感池化后，特征图中每一个格子都对应不同的颜色，注意图中只是分类那一支，且取k=3)。

#### 5.1.6 Mask R-CNN

![](1787018-20200419232816874-1006783598.png)

[2017年何恺明提出Mask R-CNN，](https://arxiv.org/abs/1703.06870)综合了很多此前优秀的研究成果，可以同时进行目标检测和实例分割。简单而言，Mask R-CNN = ResNet + FPN + Faster R-CNN + Mask，并通过RoI Align和特殊的mask损失进一步提升性能。

模型细节：

- 沿用了Faster R-CNN的思想：即利用RPN获取候选框，再映射到特征图上抠出RoI，经过池化后得到固定大小的RoI，之后就送去做分类和回归。但Mask R-CNN在RoI之后又加了另一个Mask分支，用于做实例分割。即每个RoI在池化后有两路分支head：分类和回归head、mask head。
- Backbone有多种选择，比如ResNet50、ResNet101、ResNeXt50、ResNeXt101，用network-depth-features来表示，比如常用的ResNet-50-C4，表示采用的是ResNet50上第4个卷积模块的输出特征图；另外还可以选择带或者不带FPN，比如ResNeXt-101-FPN表示采用的是ResNeXt101上FPN输出的特征图。这里只以ResNet-101-FPN为例。
- ResNet-FPN的架构，是从P2、P3、P4、P5、P6特征图引出RPN，经过后处理得到一堆的候选框，然后针对每个候选框，从P2、P3、P4、P5这四个特征图上通过公式 k=⌊k0+log2(wh√224)⌋k=⌊k0+log2(wh224)⌋ 选择一个特征图(k0k0通常选4)，来抠出对应的RoI。
- 普通的RoI池化层在两个地方做了取整操作：一是将候选框映射到特征图上抠出RoI时，一是进行RoI池化时。这对目标检测任务影响不大，但由于mask分支是像素级别的预测，因此需要更精细的坐标对应。所以将普通RoI池化层换成RoI Align层，即在这两步中保留浮点数，在池化时采用双线性插值的方法获得固定大小的池化特征图。
- 上一步得到RoI池化特征图之后，分别送到两个分支，分类和回归分支仍然用Fast R-CNN，mask分支采用FCN的方式获得像素级别的mask模板，通道数为类别个数80，其中每一层的mask都代表了对应的类别概率。(注意这里的类别概率是各个像素上每一个类别进行sigmoid激活)
- 特殊的损失函数：FCN是对每个像素进行多类别softmax分类，然后计算交叉熵损失，而这里不同。区别1：mask分支输出的80层mask，并非是80个类别进行softmax激活，而是每一个类别进行sigmoid激活；区别2：对于每一个像素来说，计算损失时，并非是直接拿80个类别的概率来计算交叉熵损失，而是这个像素属于哪个类别，就拿那个类别对应的概率值来计算二值交叉熵。这样就避免了类间竞争，让其他类别不再贡献损失(如果用softmax计算交叉熵，则其他类别会间接贡献损失)。
- 下图是两种head配置，分别是利用ResNet-C4和ResNet-FPN的backbone。

![](1787018-20200419232851311-2039647531.png)



## 六、参考资料

[1] [计算机视觉中目标检测任务脉络梳理 - 天地辽阔 - 博客园 (cnblogs.com)](https://www.cnblogs.com/inchbyinch/p/12735095.html)

[2] [(22条消息) 小目标检测综述 | 难点分析、方法汇总、数据集归纳及未来讨论_自动驾驶之心的博客-CSDN博客](https://blog.csdn.net/CV_Autobot/article/details/127274891)

[3] [[2009.00206\] RangeRCNN: Towards Fast and Accurate 3D Object Detection with Range Image Representation (arxiv.org)](https://arxiv.org/abs/2009.00206)

[4] [[2108.12817\] Airplane Detection Based on Mask Region Convolution Neural Network (arxiv.org)](https://arxiv.org/abs/2108.12817)

[5] [目标检测数据集PASCAL VOC简介 | arleyzhang](https://arleyzhang.github.io/articles/1dc20586/)

[6] http://host.robots.ox.ac.uk/pascal/VOC/

[7] https://cloud.tencent.com/developer/article/1747599

[8] https://blog.csdn.net/xingwei_09/article/details/79148294

[9] https://blog.csdn.net/hduxiejun/article/details/73199184

[10] https://blog.csdn.net/qq_41185868/article/details/82939959

[11] [(22条消息) Open Image 数据集简介_chenxp2311的博客-CSDN博客_openimage](https://blog.csdn.net/u010167269/article/details/52717394)

[12] [Introducing the Open Images Dataset – Google AI Blog (googleblog.com)](https://ai.googleblog.com/2016/09/introducing-open-images-dataset.html)